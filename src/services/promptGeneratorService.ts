import { AIService } from './aiService'
import type { ProviderConfig } from '@/stores/settingsStore'
import { useSettingsStore } from '@/stores/settingsStore'
import { promptConfigManager } from '@/config/prompts'

export class PromptGeneratorService {
  private static instance: PromptGeneratorService
  private aiService: AIService

  private constructor() {
    this.aiService = AIService.getInstance()
  }

  public static getInstance(): PromptGeneratorService {
    if (!PromptGeneratorService.instance) {
      PromptGeneratorService.instance = new PromptGeneratorService()
    }
    return PromptGeneratorService.instance
  }

  // æ ¼å¼åŒ–å˜é‡ä¸ºæç¤ºè¯éƒ¨åˆ†
  private formatVariablesForPrompt(variables: string[]): string {
    if (!variables || variables.length === 0 || variables.every(v => v.trim() === '')) {
      return ''
    }
    const nonEmptyVariables = variables.filter(v => v.trim() !== '')
    if (nonEmptyVariables.length === 0) {
      return ''
    }
    
    return `
---
Variable Integration:
The final prompt must be designed to be used in a programmatic context. As such, it needs to include specific placeholders or variables. You must incorporate the following variables into the generated prompt where it makes logical sense to do so.

Variable List:
${nonEmptyVariables.map(v => `- \`${v}\``).join('\n')}

For example, if a variable is \`{{user_topic}}\`, you might include a sentence like "The user will provide the \`{{user_topic}}\` for you to write about."
---
    `
  }

  // è·å–æµå¼æ¨¡å¼è®¾ç½®ï¼ˆä¸settingsStoreä¿æŒåŒæ­¥ï¼‰
  private getStreamMode(): boolean {
    const settingsStore = useSettingsStore()
    return settingsStore.streamMode
  }

  // è·å–ç³»ç»Ÿæç¤ºè¯å…³é”®æŒ‡ä»¤
  public async getSystemPromptThinkingPoints(
    description: string,
    model: string,
    language: string,
    variables: string[],
    provider?: ProviderConfig,
    onStreamUpdate?: (content: string) => void
  ): Promise<string[]> {
    const variablesSection = this.formatVariablesForPrompt(variables)

    // è·å–é…ç½®çš„æœ€ç»ˆæç¤ºè¯ç”Ÿæˆè§„åˆ™
    const finalRules = promptConfigManager.getFinalPromptGenerationRules()
    
    const systemContent = finalRules.THINKING_POINTS_EXTRACTION.replace('{language}', language)

    const masterPrompt = `${variablesSection}
User's Description for AI Persona:
---
${description}
---`

    const systemMessage = {
      role: 'system' as const,
      content: systemContent
    }

    const userMessage = {
      role: 'user' as const,
      content: masterPrompt
    }
    
    console.log('ğŸ” [getSystemPromptThinkingPoints] System message:', systemMessage)
    console.log('ğŸ” [getSystemPromptThinkingPoints] Messages array:', [systemMessage, userMessage])

    if (!provider) {
      throw new Error('è¯·å…ˆé…ç½®AIæä¾›å•†')
    }

    const streamMode = this.getStreamMode()
    
    // å¦‚æœæœ‰æµå¼å›è°ƒä¸”å¯ç”¨æµå¼æ¨¡å¼ï¼Œè®¾ç½®æµå¼æ›´æ–°
    if (onStreamUpdate && streamMode) {
      this.aiService.setStreamUpdateCallback((chunk: string) => {
        onStreamUpdate(chunk)
      })
    }
    
    console.log('ğŸ” [getSystemPromptThinkingPoints] Calling aiService.callAI with messages:', [systemMessage, userMessage])
    const response = await this.aiService.callAI([systemMessage, userMessage], provider, model, streamMode)
    
    // æ¸…ç†æµå¼å›è°ƒ
    if (onStreamUpdate && streamMode) {
      this.aiService.clearStreamUpdateCallback()
    }
    
    // è§£æç»“æœ
    const points = response
      .split('\n')
      .map(s => s.replace(/^[*-]\s*/, '').trim())
      .filter(Boolean)
    
    return points
  }

  // ç”Ÿæˆç³»ç»Ÿæç¤ºè¯
  public async generateSystemPrompt(
    description: string,
    model: string,
    language: string,
    variables: string[],
    thinkingPoints?: string[],
    provider?: ProviderConfig,
    onStreamUpdate?: (content: string) => void
  ): Promise<string> {
    // ä½¿ç”¨å†…ç½®çš„ç³»ç»Ÿæç¤ºè¯è§„åˆ™
    const SYSTEM_PROMPT_RULES = promptConfigManager.getSystemPromptRules()

    const variablesSection = this.formatVariablesForPrompt(variables)
    const thinkingPointsSection = (thinkingPoints && thinkingPoints.length > 0 && thinkingPoints.some(p => p.trim() !== ''))
      ? `
---
Key Directives to Incorporate:
You must intelligently integrate the following specific directives into the final System Prompt. These are non-negotiable and should guide the core logic and personality of the AI.

Directives:
${thinkingPoints.filter(p => p.trim() !== '').map(p => `- ${p}`).join('\n')}
---
      `
      : ''

    // è·å–é…ç½®çš„æœ€ç»ˆæç¤ºè¯ç”Ÿæˆè§„åˆ™
    const finalRules = promptConfigManager.getFinalPromptGenerationRules()
    
    const languageDisplay = language === 'zh' ? 'ä¸­æ–‡' : 'English'
    
    const systemContent = `${finalRules.SYSTEM_PROMPT_GENERATION}

---
Here are the prompt engineering rules I will follow:
${SYSTEM_PROMPT_RULES}
---`.replace('{language_display}', languageDisplay)

    const masterPrompt = `${variablesSection}
${thinkingPointsSection}
User's Original Description:
---
${description}
---`

    const systemMessage = {
      role: 'system' as const,
      content: systemContent
    }

    const userMessage = {
      role: 'user' as const,
      content: masterPrompt
    }

    if (!provider) {
      throw new Error('è¯·å…ˆé…ç½®AIæä¾›å•†')
    }

    const streamMode = this.getStreamMode()
    
    // å¦‚æœæœ‰æµå¼å›è°ƒä¸”å¯ç”¨æµå¼æ¨¡å¼ï¼Œè®¾ç½®æµå¼æ›´æ–°
    if (onStreamUpdate && streamMode) {
      this.aiService.setStreamUpdateCallback((chunk: string) => {
        onStreamUpdate(chunk)
      })
    }
    
    const response = await this.aiService.callAI([systemMessage, userMessage], provider, model, streamMode)
    
    // æ¸…ç†æµå¼å›è°ƒ
    if (onStreamUpdate && streamMode) {
      this.aiService.clearStreamUpdateCallback()
    }
    
    // æ¸…ç†markdownä»£ç å—æ ¼å¼
    let cleaned = response.replace(/```/g, '').trim()
    
    // å¦‚æœå¼€å¤´æœ‰"markdown"å­—ç¬¦ï¼Œç§»é™¤å®ƒ
    if (cleaned.startsWith('markdown\n')) {
      cleaned = cleaned.substring(9) // ç§»é™¤"markdown\n"
    } else if (cleaned.startsWith('markdown')) {
      cleaned = cleaned.substring(8) // ç§»é™¤"markdown"
    }
    
    return cleaned.trim()
  }

  // è·å–ä¼˜åŒ–å»ºè®®
  public async getOptimizationAdvice(
    promptToAnalyze: string,
    promptType: 'system' | 'user',
    model: string,
    language: string,
    variables: string[],
    provider?: ProviderConfig,
    onStreamUpdate?: (content: string) => void
  ): Promise<string[]> {
    const variablesSection = this.formatVariablesForPrompt(variables)

    // è·å–é…ç½®çš„æœ€ç»ˆæç¤ºè¯ç”Ÿæˆè§„åˆ™
    const finalRules = promptConfigManager.getFinalPromptGenerationRules()
    
    const promptTypeCapitalized = promptType.charAt(0).toUpperCase() + promptType.slice(1)
    
    const systemContent = finalRules.OPTIMIZATION_ADVICE_GENERATION.replace('{promptType}', promptType).replace('{language}', language)

    const masterPrompt = `${variablesSection}
${promptTypeCapitalized} Prompt to Analyze:
---
${promptToAnalyze}
---`

    const systemMessage = {
      role: 'system' as const,
      content: systemContent
    }

    const userMessage = {
      role: 'user' as const,
      content: masterPrompt
    }

    if (!provider) {
      throw new Error('è¯·å…ˆé…ç½®AIæä¾›å•†')
    }

    const streamMode = this.getStreamMode()
    
    // å¦‚æœæœ‰æµå¼å›è°ƒä¸”å¯ç”¨æµå¼æ¨¡å¼ï¼Œè®¾ç½®æµå¼æ›´æ–°
    if (onStreamUpdate && streamMode) {
      this.aiService.setStreamUpdateCallback((chunk: string) => {
        onStreamUpdate(chunk)
      })
    }
    
    const response = await this.aiService.callAI([systemMessage, userMessage], provider, model, streamMode)
    
    // æ¸…ç†æµå¼å›è°ƒ
    if (onStreamUpdate && streamMode) {
      this.aiService.clearStreamUpdateCallback()
    }
    
    // è§£æç»“æœ
    const advice = response
      .split('\n')
      .map(s => s.replace(/^[*-]\s*/, '').trim())
      .filter(Boolean)
    
    return advice
  }

  // åº”ç”¨ä¼˜åŒ–å»ºè®®
  public async applyOptimizationAdvice(
    originalPrompt: string,
    advice: string[],
    promptType: 'system' | 'user',
    model: string,
    language: string,
    variables: string[],
    provider?: ProviderConfig,
    onStreamUpdate?: (content: string) => void
  ): Promise<string> {
    // ä½¿ç”¨å†…ç½®çš„ç³»ç»Ÿæç¤ºè¯è§„åˆ™
    const SYSTEM_PROMPT_RULES = promptConfigManager.getSystemPromptRules()

    const variablesSection = this.formatVariablesForPrompt(variables)
    const adviceSection = advice.map(a => `- ${a}`).join('\n')

    // è·å–é…ç½®çš„æœ€ç»ˆæç¤ºè¯ç”Ÿæˆè§„åˆ™
    const finalRules = promptConfigManager.getFinalPromptGenerationRules()
    
    const languageDisplay = language === 'zh' ? 'ä¸­æ–‡' : 'English'
    const promptTypeCapitalized = promptType.charAt(0).toUpperCase() + promptType.slice(1)
    
    const systemContent = `${finalRules.OPTIMIZATION_APPLICATION}

---
Here are the core principles of elite prompt engineering I will follow:
${SYSTEM_PROMPT_RULES}
---`.replace('{promptType}', promptType)
      .replace('{language_display}', languageDisplay)
      .replace('{promptType_capitalized}', promptTypeCapitalized)

    const masterPrompt = `${variablesSection}
Original ${promptTypeCapitalized} Prompt:
---
${originalPrompt}
---

Optimization Suggestions to Apply:
---
${adviceSection}
---`

    const systemMessage = {
      role: 'system' as const,
      content: systemContent
    }

    const userMessage = {
      role: 'user' as const,
      content: masterPrompt
    }
    
    console.log('ğŸ” [applyOptimizationAdvice] System message:', systemMessage)
    console.log('ğŸ” [applyOptimizationAdvice] Messages array:', [systemMessage, userMessage])

    if (!provider) {
      throw new Error('è¯·å…ˆé…ç½®AIæä¾›å•†')
    }

    const streamMode = this.getStreamMode()
    
    // å¦‚æœæœ‰æµå¼å›è°ƒä¸”å¯ç”¨æµå¼æ¨¡å¼ï¼Œè®¾ç½®æµå¼æ›´æ–°
    if (onStreamUpdate && streamMode) {
      this.aiService.setStreamUpdateCallback((chunk: string) => {
        onStreamUpdate(chunk)
      })
    }
    
    console.log('ğŸ” [applyOptimizationAdvice] Calling aiService.callAI with messages:', [systemMessage, userMessage])
    const response = await this.aiService.callAI([systemMessage, userMessage], provider, model, streamMode)
    
    // æ¸…ç†æµå¼å›è°ƒ
    if (onStreamUpdate && streamMode) {
      this.aiService.clearStreamUpdateCallback()
    }
    
    // æ¸…ç†markdownä»£ç å—æ ¼å¼
    let cleaned = response.replace(/```/g, '').trim()
    
    // å¦‚æœå¼€å¤´æœ‰"markdown"å­—ç¬¦ï¼Œç§»é™¤å®ƒ
    if (cleaned.startsWith('markdown\n')) {
      cleaned = cleaned.substring(9) // ç§»é™¤"markdown\n"
    } else if (cleaned.startsWith('markdown')) {
      cleaned = cleaned.substring(8) // ç§»é™¤"markdown"
    }
    
    return cleaned.trim()
  }
}